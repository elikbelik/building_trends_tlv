{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/elad/workspace/playground/stuff/horn_kedar')\n",
    "\n",
    "from load_data import load_data, create_dataset, get_dataset_tik_ids\n",
    "from trainer import Trainer, ExperimentResults\n",
    "from parameters import HyperParams, PosWeightSchedule, NUM_FIELDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "dataset_train, dataset_val, dataset_test = create_dataset(data)\n",
    "data_triplet = (dataset_train, dataset_val, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS_FOLDER = './experiments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(hyperparams: HyperParams, data_triplet = data_triplet, try_load: bool = False, avoid_save: bool = False, verbose: bool = True):\n",
    "    dataset_train, dataset_val, dataset_test = data_triplet\n",
    "    current_experiment_path = os.path.join(EXPERIMENTS_FOLDER, hyperparams.experiment_name)\n",
    "    if try_load and os.path.exists(current_experiment_path):\n",
    "        res = ExperimentResults.load(current_experiment_path)\n",
    "        trainer = Trainer.from_experiment_results(res)\n",
    "    else:\n",
    "        trainer = Trainer(hyperparams)\n",
    "        losses = trainer.train(dataset_train, dataset_val, verbose=verbose)\n",
    "        pr_curve = trainer.get_precision_recall_curve(dataset_test)\n",
    "        res = ExperimentResults(\n",
    "            hyperparams=hyperparams,\n",
    "            model=trainer.best_model,\n",
    "            losses=losses,\n",
    "            precision_recall_curve=pr_curve,\n",
    "            latest_model=trainer.latest_model,\n",
    "        )\n",
    "        if not avoid_save:\n",
    "            res.save(EXPERIMENTS_FOLDER)\n",
    "    return res, trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = HyperParams(\n",
    "    experiment_name='VERSION01_bignet_pos_weight_20_weight_decay',\n",
    "    scheduler='cosine',\n",
    "    pos_weight=20,\n",
    "    num_rnn_layers=2,\n",
    "    hidden_size=50,\n",
    "    weight_decay=1e-3,\n",
    "    use_max_metric='f1_pm0',\n",
    ")\n",
    "\n",
    "res, trainer = run_train(hyperparams)\n",
    "\n",
    "res.plot_losses()\n",
    "res.plot_precision_recall_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = HyperParams(\n",
    "    experiment_name='VERSION02_v1_pos_weight_schedule_num_epochs_20k',\n",
    "    scheduler='cosine',\n",
    "    pos_weight=10,\n",
    "    num_rnn_layers=2,\n",
    "    hidden_size=50,\n",
    "    weight_decay=1e-3,\n",
    "    learning_rate=0.001,\n",
    "    use_max_metric='f1_pm0',\n",
    "    pos_weight_schedule=[\n",
    "        PosWeightSchedule(num_epochs=5000, pos_weight=100),\n",
    "        PosWeightSchedule(num_epochs=5000, pos_weight=20),\n",
    "        PosWeightSchedule(num_epochs=5000, pos_weight=10),\n",
    "        PosWeightSchedule(num_epochs=5000, pos_weight=5),\n",
    "    ],\n",
    "    num_epochs=20000,\n",
    "    start_model_path=os.path.join(EXPERIMENTS_FOLDER, 'VERSION01_bignet_pos_weight_20_weight_decay'),\n",
    ")\n",
    "\n",
    "res, trainer = run_train(hyperparams)\n",
    "\n",
    "res.plot_losses()\n",
    "res.plot_precision_recall_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExperimentResults.plot_multiple_pr_curves(EXPERIMENTS_FOLDER, experiment_names=['VERSION01_bignet_pos_weight_20_weight_decay', 'VERSION02_v1_pos_weight_schedule_num_epochs_20k'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import create_unlabeled_dataset, load_unlabeled_data\n",
    "\n",
    "unlabeled_data = load_unlabeled_data()\n",
    "unlabeled_dataset, tik_ids = create_unlabeled_dataset(unlabeled_data)\n",
    "\n",
    "len(unlabeled_dataset), len(tik_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'VERSION02_v1_pos_weight_schedule_num_epochs_20k'\n",
    "threshold = 0.99\n",
    "res = ExperimentResults.load(os.path.join(EXPERIMENTS_FOLDER, experiment_name))\n",
    "trainer = Trainer.from_experiment_results(res)\n",
    "predictions_df = trainer.infer_to_df(unlabeled_dataset, tik_ids, threshold=threshold)\n",
    "predictions_df.to_csv(os.path.join(EXPERIMENTS_FOLDER, f'{experiment_name}_threshold_{int(threshold*100)}_predictions.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_path = 'data/document type.csv'\n",
    "fields_df = pd.read_csv(fields_path)\n",
    "fields_names = fields_df['Document_type'].tolist()\n",
    "fields_names_reversed = [x[::-1] for x in fields_names]\n",
    "fields_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'VERSION02_v1_pos_weight_schedule_num_epochs_20k'\n",
    "res = ExperimentResults.load(os.path.join(EXPERIMENTS_FOLDER, model_name))\n",
    "trainer = Trainer.from_experiment_results(res)\n",
    "params = list(trainer.best_model.named_parameters())\n",
    "p = dict(params)\n",
    "\n",
    "# Print names and shapes\n",
    "for name, param in trainer.best_model.named_parameters():\n",
    "    print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc the contribution of each element in x_t to the output\n",
    "layer_0 = torch.cat([p['rnn.weight_ih_l0'], p['rnn.weight_ih_l0_reverse']], dim=0)\n",
    "d_out_d_x_t = p['fc.weight'] @ torch.cat([p['rnn.weight_ih_l1'] @ layer_0, p['rnn.weight_ih_l1_reverse'] @ layer_0], dim=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "plt.bar(range(NUM_FIELDS), d_out_d_x_t.detach().numpy().squeeze())\n",
    "plt.xticks(range(NUM_FIELDS), fields_names_reversed, rotation=90)\n",
    "plt.title('Contribution of each element in current time step to the output')\n",
    "plt.xlabel('Field index')\n",
    "plt.ylabel('Contribution')\n",
    "ax.grid(True, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc the contribution of each element in x_t-1 to the output\n",
    "part1 = p['rnn.weight_hh_l1'] @ p['rnn.weight_ih_l1'] @ torch.cat([p['rnn.weight_ih_l0'], p['rnn.weight_ih_l0_reverse']], dim=0)\n",
    "part1_cat = torch.cat([part1, torch.zeros_like(part1)], dim=0)\n",
    "part2 = p['rnn.weight_hh_l0'] @ p['rnn.weight_ih_l0']\n",
    "part2_cat = torch.cat([part2, torch.zeros_like(part2)], dim=0)\n",
    "part2_cat_cat = torch.cat([p['rnn.weight_ih_l1'] @ part2_cat, p['rnn.weight_ih_l1_reverse'] @ part2_cat], dim=0)\n",
    "d_out_d_x_t_minus_1 = p['fc.weight'] @ (part1_cat + part2_cat_cat)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "plt.bar(range(NUM_FIELDS), d_out_d_x_t_minus_1.detach().numpy().squeeze())\n",
    "plt.xticks(range(NUM_FIELDS), fields_names_reversed, rotation=90)\n",
    "plt.title('Contribution of each element in previous time step to the output')\n",
    "plt.xlabel('Field index')\n",
    "plt.ylabel('Contribution')\n",
    "ax.grid(True, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc the contribution of each element in x_t+1 to the output\n",
    "part1 = p['rnn.weight_hh_l1_reverse'] @ p['rnn.weight_ih_l1_reverse'] @ torch.cat([p['rnn.weight_ih_l0'], p['rnn.weight_ih_l0_reverse']], dim=0)\n",
    "part1_cat = torch.cat([torch.zeros_like(part1), part1], dim=0)\n",
    "part2 = p['rnn.weight_hh_l0_reverse'] @ p['rnn.weight_ih_l0_reverse']\n",
    "part2_cat = torch.cat([torch.zeros_like(part2), part2], dim=0)\n",
    "part2_cat_cat = torch.cat([p['rnn.weight_ih_l1'] @ part2_cat, p['rnn.weight_ih_l1_reverse'] @ part2_cat], dim=0)\n",
    "d_out_d_x_t_plus_1 = p['fc.weight'] @ (part1_cat + part2_cat_cat)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "plt.bar(range(NUM_FIELDS), d_out_d_x_t_plus_1.detach().numpy().squeeze())\n",
    "plt.xticks(range(NUM_FIELDS), fields_names_reversed, rotation=90)\n",
    "plt.title('Contribution of each element in next time step to the output')\n",
    "plt.xlabel('Field index')\n",
    "plt.ylabel('Contribution')\n",
    "ax.grid(True, axis='x')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground-81CyFrGf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
